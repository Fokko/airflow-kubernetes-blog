{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are excited to announce a Kubernetes Operator to increase airflow's viability as a job orchestration engine using the power of the Kubernetes cloud deployment framework. \n",
    "\n",
    "Since its inception, airflow's power as a job orchestrator has been its flexibility. Airflow offers a wide range of native operators (for services such as spark, hbase, etc.) while also offering easy extensibility through its plugin framework. However, one limitation of the Apache Airflow project has been that it has always relied on previously running systems. \n",
    "\n",
    "Over the next few months, we hope to change this model using containerization. With a kubernetes operator, airflow is no longer limited by the size of the worker nodes, or the libararies loaded to the airflow instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is kubernetes? (Is this section necessary?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kubernetes is a container orchestration system\n",
    "* users can launch docker containers on custom sized instnaces\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kubernetes Operator: The \"Whatever-your-heart-desires\" Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Running a basic container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first example, let's create a basic docker container that runs simple python commands. This container should be able to pass and fail at will, and record logs that show up in the airflow terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def fail():\n",
    "    return 10 / 0\n",
    "\n",
    "\n",
    "def succeed():\n",
    "    return 10 / 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    arg = sys.argv[1]\n",
    "    if arg == 'pass':\n",
    "        succeed()\n",
    "    elif arg == 'fail':\n",
    "        fail()\n",
    "    else:\n",
    "        print('invalid command: {}'.format(arg))\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph discussing creating a docker container for running this scipy code using an entry point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM ubuntu:16.04\n",
    "\n",
    "# install python dependencies\n",
    "RUN apt-get update -y && apt-get install -y \\\n",
    "        wget \\\n",
    "        python-dev \\\n",
    "        python-pip \\\n",
    "        libczmq-dev \\\n",
    "        libcurlpp-dev \\\n",
    "        curl \\\n",
    "        libssl-dev \\\n",
    "        git \\\n",
    "        inetutils-telnet \\\n",
    "        bind9utils\n",
    "\n",
    "RUN pip install -U setuptools && \\\n",
    "    pip install -U pip\n",
    "\n",
    "\n",
    "COPY airflow-example.py /airflow-example.py\n",
    "COPY entrypoint.sh /entrypoint.sh\n",
    "RUN chmod +x /entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"/entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of entrypoint files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "python /airflow-example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAG File that creates two kubernetes operators, one that passes and one that fails\n",
    "\n",
    "passing = KubernetesPodOperator(namespace='default',\n",
    "                          image=\"airflow-test\",\n",
    "                          arguments=[\"succeed\"],\n",
    "                          labels={\"foo\": \"bar\"},\n",
    "                          name=\"passing-test\",\n",
    "                          task_id=\"task1\"\n",
    "                          )\n",
    "\n",
    "failing = KubernetesPodOperator(namespace='default',\n",
    "                          image=\"airflow-test\",\n",
    "                          arguments=[\"succeed\"],\n",
    "                          labels={\"foo\": \"bar\"},\n",
    "                          name=\"fail\",\n",
    "                          task_id=\"task1\"\n",
    "                          )\n",
    "\n",
    "failing.set_upstream(passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will eventually be a series of images about the running DAGs\n",
    "\n",
    "<img src=\"files/image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to github file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Running a model using scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Running a kubeflow pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph about kubeflow and how awesome it will be\n",
    "\n",
    "* Developed by Google and Bloomberg to make kubernetes data development easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final statements about all the possibilities this opens up\n",
    "\n",
    "* Airflow Kubernetes Executor\n",
    "* Custom Deployments via python API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
