{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Today, we are excited to announce a Kubernetes Operator to increase airflow's\n",
    "viability as a job orchestration engine using the power of the Kubernetes cloud\n",
    "deployment framework. \n",
    "\n",
    "Since its inception, airflow's power as a job\n",
    "orchestrator has been its flexibility. Airflow offers a wide range of native\n",
    "operators (for services such as spark, hbase, etc.) while also offering easy\n",
    "extensibility through its plugin framework. However, one limitation of the\n",
    "Apache Airflow project is that...\n",
    "\n",
    "Over the next few months, we will be offering\n",
    "a series of kubernetes-based offerings that will vastly expand airflows' native\n",
    "capabilities. With the addition of a Kubernetes Operator, users will be able to\n",
    "launch arbitrary docker containers with customizable resources, secrets, and...\n",
    "\n",
    "## What is kubernetes? (Is this section necessary?)\n",
    "\n",
    "[Kubernetes](https://kubernetes.io/) is an open-source container deployment\n",
    "engine released by Google. Based on google's own\n",
    "[Borg](http://blog.kubernetes.io/2015/04/borg-predecessor-to-kubernetes.html),\n",
    "kubernetes allows for easy deployment of images using a highly flexible API.\n",
    "Using kubernetes you can [deploy spark jobs](https://github.com/apache-spark-\n",
    "on-k8s/spark), launch end-to-end applications, or ... using yaml files, python,\n",
    "golang, or java bindings. The kubernetes API's programatic launching of\n",
    "containers seemed a perfect marriage with Airflow's \"code as configuration\"\n",
    "philosophy.\n",
    "\n",
    "# The Kubernetes Operator: The \"Whatever-your-heart-desires\" Operator\n",
    "\n",
    "As DevOps\n",
    "pioneers, we are always looking for ways to make our deployments and ETL\n",
    "pipelines simpler to manage. Any opportunity to reduce the number of moving\n",
    "parts in our codebase will always lead to future opportunities to break in the\n",
    "future. The following are a list of benefits the Kubernetes Operator in reducing\n",
    "the Airflow Engineer's footprint\n",
    "\n",
    "* **Increased flexibility for deployments**\n",
    "Airflow's plugin API has always offered a significant boon to engineers wishing\n",
    "to test new functionalities within their DAGS, however it has always had the\n",
    "downside that to create a new operator, one must develop an entirely new plugin.\n",
    "Now any task that can be run within a docker container is accessible through the\n",
    "same same operator with no extra airflow code to maintain.\n",
    "* **Flexibility of\n",
    "configurations and dependencies** For operators that are run within static\n",
    "airflow workers, dependency management can become quite difficult. If I want to\n",
    "run one task that requires scipy, and another that requires numpy, I have to\n",
    "either maintain both dependencies within my airflow worker, or somehow configure\n",
    "* **Usage of kubernetes secrets for added security** Handling sensitive data is\n",
    "a core responsibility of any devops engineer. At every opportunity, we want to\n",
    "minimize any API keys, database passwords,  or ... to a strict need-to-know\n",
    "basis. With the kubernetes operator, we can use the kubernetes Vault technology\n",
    "to store all sensitive data. This means that the airflow workers will never have\n",
    "access to this information, and can simply request that pods be built with only\n",
    "the secrets they need\n",
    "\n",
    "# Examples\n",
    "\n",
    "## Example 1: Running a basic container\n",
    "\n",
    "For this first example, let's create a basic docker image that runs simple\n",
    "python commands. This example will only have two end-results: Succeed or fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def succeed():\n",
    "    return 10 / 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    arg = sys.argv[1]\n",
    "    if arg == 'pass':\n",
    "        succeed()\n",
    "    else:\n",
    "        print('invalid command: {}'.format(arg))\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code, we'll create a DockerFile and an `entrypoint.sh` file that\n",
    "will run our basic python script (while this entrypoint script is pretty simple\n",
    "at the moment, we will later see how it can expand to more complex use-cases)\n",
    "\n",
    "#### Entrypoint.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "python /airflow-example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FROM ubuntu:16.04\n",
    "\n",
    "# install python dependencies\n",
    "RUN apt-get update -y && apt-get install -y \\\n",
    "        wget \\\n",
    "        python-dev \\\n",
    "        python-pip \\\n",
    "        libczmq-dev \\\n",
    "        libcurlpp-dev \\\n",
    "        curl \\\n",
    "        libssl-dev \\\n",
    "        git \\\n",
    "        inetutils-telnet \\\n",
    "        bind9utils\n",
    "\n",
    "RUN pip install -U setuptools && \\\n",
    "    pip install -U pip\n",
    "\n",
    "\n",
    "COPY airflow-example.py /airflow-example.py\n",
    "COPY entrypoint.sh /entrypoint.sh\n",
    "RUN chmod +x /entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"/entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create a simple DAG file that runs a passing and failing\n",
    "version of our example script. This script can be run on any executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Airflow DAG File that creates two kubernetes operators, one that passes and one that fails\n",
    "from airflow import DAG\n",
    "from airflow.operators import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.operators.docker_operator import DockerOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime.utcnow(),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'kubernetes_sample', default_args=default_args, schedule_interval=timedelta(minutes=10))\n",
    "\n",
    "passing = KubernetesPodOperator(namespace='default',\n",
    "                          image=\"airflow-test\",\n",
    "                          arguments=[\"succeed\"],\n",
    "                          labels={\"foo\": \"bar\"},\n",
    "                          name=\"passing-test\",\n",
    "                          task_id=\"task1\",\n",
    "                          dag=dag\n",
    "                          )\n",
    "\n",
    "failing = KubernetesPodOperator(namespace='default',\n",
    "                          image=\"airflow-test\",\n",
    "                          arguments=[\"fail\"],\n",
    "                          labels={\"foo\": \"bar\"},\n",
    "                          name=\"fail\",\n",
    "                          task_id=\"task1\",\n",
    "                          dag=dag\n",
    "                          )\n",
    "\n",
    "failing.set_upstream(passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will eventually be a series of images about the running DAGs\n",
    "\n",
    "<img\n",
    "src=\"files/image.png\">\n",
    "\n",
    "Link to github file\n",
    "\n",
    "## Example 2: Running a model using scipy\n",
    "\n",
    "## Example 3: Running a kubeflow pipeline\n",
    "\n",
    "Paragraph about kubeflow and how awesome it will be\n",
    "\n",
    "* Developed by Google and\n",
    "Bloomberg to make kubernetes data development easier\n",
    "\n",
    "# Architecture\n",
    "\n",
    "<img src=\"architecture.png\">\n",
    "\n",
    "The architecture for the kubernetes operator is\n",
    "pretty straightforward. The operator neds to request a pod from kubernetes,\n",
    "monitor said pod (while blocking further actions until the pod has completed),\n",
    "and\n",
    "\n",
    "# Closing Statements\n",
    "\n",
    "Final statements about all the possibilities this opens up\n",
    "\n",
    "* Airflow Kubernetes\n",
    "Executor\n",
    "* Custom Deployments via python API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
